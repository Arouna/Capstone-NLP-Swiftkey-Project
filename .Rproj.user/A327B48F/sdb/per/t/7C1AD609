{
    "contents" : "#' @description\n#' Fits an N-Gram language model that can be used for type ahead language\n#' prediction.\n#'\n#' @details\n#' Fits an N-Gram language model that conditions the probability of the next\n#' word in a phrase based on the previous 'N-1' words.  The previous 'N-1' words\n#' in a phrase is known as the history.  The length of the history depends on 'N'.\n#' A bigram (2-gram) model leverages only the previous word as the history. A\n#' trigram (3-gram) model considers the previous 2 words. A 4-gram model considers\n#' the previous 3 words.\n#'\n#' The maximum liklihood estimate for the probability of a word is calculated as the\n#' number of times the phrase occurred in a training corpus divided by the number of\n#' times the history occurred.  Consider the phrase \"I Love You\" and a trigram (3-gram)\n#' model.  The probability of the word \"You\" following the history \"I Love\" is\n#' calculated as follows.  The number of times \"I Love You\" occurs in the training\n#' corpus is divided by the number of times \"I Love\" occurs.\n#'\n#' Having calculated the probability of each word for all historys discovered in the\n#' training corpus, the model is able to provide a prediction for the next most likely\n#' word.  The model is provided input and searches for a history matching this. The\n#' model then chooses the next word with the greatest calculated probability.\n#'\n#' N-gram models with a larger value of 'N' account for greater history and tend to\n#' be more accurate.  These same models also tend to encounter N-grams in practice\n#' that were not included in the training corpus.  When this occurs the model has no\n#' basis to make a prediction.\n#'\n#' This model uses the Katz Back-off algorithm to balances these pros and cons. The\n#' model uses the training corpus to create multiple internal models with different\n#' values of N.  For example, a model may include unigrams, bigrams, and trigrams.\n#' The model first consult the model with the greatest value of 'N'.  If the model\n#' is unable to provide a prediction the model with the next greatest value of 'N'\n#' is consulted.  The prediction is provided by the highest order model with sufficient\n#' history to make a valid prediction.\n#'\n#' @param ngram The data table use to generate the language model.\n#' @param N The type of N-Gram models to fit; unigram, bigram, trigram, etc.\n#' @param freq_cutoff All N-grams that occur lesser than the cutoff are removed\n#'      from the model.\n#' @param rank_cutoff The lowest rank to keep in the model.  If only the model's\n#'      top 5 suggestions are used, then set the rank_cutoff to 5.\n#' @param delimiter The delimiters used to define word boundaries.\n#' @return An object of class 'ngram' that can be used to perform type ahead text\n#'      prediction.\n#\n#'\ngenmodel <- function (ngrams,\n                        N           = 1:3,\n                        freq_cutoff = 1,\n                        rank_cutoff = 5,\n                        delimiters  = ' \\r\\n\\t.,;:\\\\\"()?!',\n                        start_tag = \"debutdephrase\", \n                        end_tag = \"findephrase\") {\n  \n  # sanity checks\n  stopifnot (is.numeric (N))\n  stopifnot (is.numeric (freq_cutoff))\n  stopifnot (is.numeric (rank_cutoff))\n  \n  # extract the history and the next word for each ngram\n  ngrams [, word    := last_word (phrase),        by = phrase]\n  ngrams [, history := extract_history (phrase), by = phrase]\n  \n  # calculate the MLE of the probability of occurrence for each n-gram\n  ngrams <- calprob (ngrams)\n  \n  # exclude ngrams that are below the frequency cut-off\n  ngrams <- ngrams [ history_frequency >= freq_cutoff, list (phrase, history, word, p) ]\n  \n  # do not predict a 'start of sentence'\n  ngrams <- ngrams [word != \"debutdephrase\"]\n  \n  # do not predict 'end of sentence' with no history or at the start of a sentence\n  ngrams <- ngrams [!(history == \"\"  & word == \"findephrase\")]\n  ngrams <- ngrams [!(history == \"debutdephrase\" & word == \"findephrase\")]\n  \n  # mark each n-gram as a 1, 2, ... N gram\n  regex <- paste0 (\"[\", delimiters, \"]+\")\n  ngrams [, n := unlist (lapply (stri_split (phrase, regex = regex), length)) ]\n  \n  # keep only most likely words for each history\n  ngrams <- ngrams [ order (history, -p)]\n  ngrams [, rank := 1:.N, by = history]\n  ngrams <- ngrams [ rank <= rank_cutoff ]\n  \n  # create a container for the model\n  model <- list (ngrams      = ngrams,\n                 N           = N,\n                 freq_cutoff = freq_cutoff,\n                 rank_cutoff = rank_cutoff)\n  class (model) <- \"ngram\"\n  \n  return (model)\n}\n",
    "created" : 1460821688105.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2788492354",
    "id" : "7C1AD609",
    "lastKnownWriteTime" : 1461012511,
    "path" : "C:/Users/amopa/Desktop/Coursera/DataSciences/Capstone NLP Swiftkey Project/finalR/genmodel.R",
    "project_path" : "finalR/genmodel.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "type" : "r_source"
}